{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification Using FLAX with/without MC dropout.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Libraries"
      ],
      "metadata": {
        "id": "rAqxSJoRTa1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try :\n",
        "  import jax\n",
        "except ModuleNotFoundError:\n",
        "  %pip install --upgrade -qq pip jax jaxlib\n",
        "  import jax\n",
        "try :\n",
        "  import flax\n",
        "except ModuleNotFoundError:\n",
        "  %pip install --upgrade -qq git+https://github.com/google/flax.git\n",
        "  import flax\n",
        "\n",
        "from jax import lax, random, numpy as jnp\n",
        "import sklearn\n",
        "from jax import grad\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from flax.core import freeze, unfreeze\n",
        "from flax import linen as nn\n",
        "import numpy as np\n",
        "import optax\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from jax.config import config\n",
        "config.enable_omnistaging()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik9i03iJUMGx",
        "outputId": "92658136-d231-4843-ae92-b899633a8951"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/config.py:180: UserWarning: enable_omnistaging() is a no-op in JAX versions 0.2.12 and higher;\n",
            "see https://github.com/google/jax/blob/main/design_notes/omnistaging.md\n",
            "  \"enable_omnistaging() is a no-op in JAX versions 0.2.12 and higher;\\n\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialisatiion of Global variables"
      ],
      "metadata": {
        "id": "nNDttGF6ys4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define number of Samples\n",
        "n_samples = 10000\n",
        "\n",
        "#Initialise learning rate\n",
        "lr = 0.1\n",
        "\n",
        "#Initialise number of epochs \n",
        "epochs = 75\n",
        "\n",
        "#Initialise loss at which epoch is to be printed (In this case it will be multiples of 5)\n",
        "log_period_epoch = 5\n",
        "\n",
        "#Initialise number of classes\n",
        "number_of_class = 3 #2 for make_moons and 3 for make_blobs\n",
        "\n",
        "#Number of Neurons in input layer\n",
        "layer1 = 2\n",
        "\n",
        "#Number of Neurons in output layer\n",
        "layer2 = number_of_class\n",
        "\n",
        "#Dimensions of weights and biases\n",
        "x_dim = 2  \n",
        "y_dim = number_of_class\n",
        "\n",
        "#Can be any number\n",
        "RS = 0\n",
        "seed = 23"
      ],
      "metadata": {
        "id": "zv68xSX4ysQl"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function to convert data into categories"
      ],
      "metadata": {
        "id": "1t2bvczl-Qnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(x, k, dtype=np.float32):\n",
        "    \"\"\"Create a one-hot encoding of x of size k \"\"\"\n",
        "    return np.array(x[:, None] == np.arange(k), dtype)"
      ],
      "metadata": {
        "id": "5vUi-pqh-a7c"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defifining MC Dropout \n"
      ],
      "metadata": {
        "id": "4PgY7ukqKCTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MCDropout(nn.Dropout):\n",
        "  def call(self, inputs):\n",
        "    return super().call(inputs, training = True)"
      ],
      "metadata": {
        "id": "s0kskHJRKBbo"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MLP using FLAX with/without MC Dropout"
      ],
      "metadata": {
        "id": "RLwMA9x4y5q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Uncomment the line below for using make_moons \n",
        "#X,y = sklearn.datasets.make_moons(n_samples=n_samples, shuffle=True,random_state=None)\n",
        "\n",
        "#Uncomment the line below for using make_blobs\n",
        "X,y = sklearn.datasets.make_blobs(n_samples=n_samples, shuffle=True,random_state=None)\n",
        "\n",
        "n_feat = X.shape[1]\n",
        "\n",
        "#Dividing into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=RS\n",
        ")\n",
        "\n",
        "#Using one hot encoding to divide output into classes\n",
        "y_train = one_hot(y_train, number_of_class)\n",
        "y_test = one_hot(y_test, number_of_class)\n"
      ],
      "metadata": {
        "id": "dI4fBmHey4qK"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Multi Layer Perceptron\n",
        "class mlp(nn.Module): \n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(features=layer1, use_bias = True)(x)\n",
        "        x = nn.relu(x)\n",
        "\n",
        "        #Uncomment line below to use MC Dropout\n",
        "        #x = MCDropout(rate=0.2)(x, deterministic=True)\n",
        "\n",
        "        x = nn.Dense(features=layer2, use_bias = True)(x)\n",
        "        x = nn.softmax(x)\n",
        "        return x\n",
        "      \n",
        "model = mlp()"
      ],
      "metadata": {
        "id": "OnKOdiosyjl0"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs = X_train\n",
        "ys = y_train\n",
        "\n",
        "#Initialising weights and biases randomly\n",
        "key, w_key, b_key = random.split(random.PRNGKey(seed), num=3)\n",
        "W = random.normal(w_key, (x_dim, y_dim))  # weight\n",
        "b = random.normal(b_key, (y_dim,))  # bias\n",
        "true_params = freeze({'params': {'bias': b, 'kernel': W}})\n"
      ],
      "metadata": {
        "id": "RxA4xfPOzP2G"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Loss function as binary cross entropy\n",
        "def binary_cross_entropy(xs, ys):   \n",
        "    def temp_cross_entropy(params):\n",
        "        def temp(x, y):\n",
        "            pred = model.apply(params, x)           \n",
        "            #Uncomment the line below for using make_moons \n",
        "            #temp_loss = -y[:,0]*jnp.log(pred[:,0])-y[:,1]*jnp.log(pred[:,1])\n",
        "\n",
        "            #Uncomment the line below for using make_blobs\n",
        "            temp_loss = -y[:,0]*jnp.log(pred[:,0])-y[:,1]*jnp.log(pred[:,1])-y[:,2]*jnp.log(pred[:,2])\n",
        "            return temp_loss\n",
        "        return jnp.mean(temp(xs, ys))\n",
        "    return jax.jit(temp_cross_entropy)  \n",
        "\n",
        "loss = binary_cross_entropy(xs, ys)\n",
        "value_and_grad_fn = jax.value_and_grad(loss)"
      ],
      "metadata": {
        "id": "VKfyLgcW2uYB"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training without optax"
      ],
      "metadata": {
        "id": "c5yaN9SZPeUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = model.init(key, xs)\n",
        "for epoch in range(epochs):\n",
        "    loss, grads = value_and_grad_fn(params)\n",
        "    params = jax.tree_multimap(lambda p, g: p - lr * g, params, grads)\n",
        "    if epoch % log_period_epoch == 0:\n",
        "        print(f'epoch {epoch}, loss = {loss}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv6WTNqH3UXm",
        "outputId": "4b38c8b4-baf6-4483-9fbc-962475b94cd8"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/tree_util.py:189: FutureWarning: jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() instead as a drop-in replacement.\n",
            "  'instead as a drop-in replacement.', FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss = 2.889653205871582\n",
            "epoch 5, loss = 0.5823025703430176\n",
            "epoch 10, loss = 0.41217637062072754\n",
            "epoch 15, loss = 0.3339276611804962\n",
            "epoch 20, loss = 0.2888086438179016\n",
            "epoch 25, loss = 0.2569538950920105\n",
            "epoch 30, loss = 0.23192162811756134\n",
            "epoch 35, loss = 0.2112276554107666\n",
            "epoch 40, loss = 0.1936613917350769\n",
            "epoch 45, loss = 0.17850534617900848\n",
            "epoch 50, loss = 0.16529275476932526\n",
            "epoch 55, loss = 0.15367868542671204\n",
            "epoch 60, loss = 0.14340732991695404\n",
            "epoch 65, loss = 0.1342749148607254\n",
            "epoch 70, loss = 0.12611515820026398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting output of test dataset\n",
        "y_pred = model.apply(params, X_test)\n",
        "\n",
        "#Rounding off the probabilities\n",
        "y_pred = jnp.where(y_pred < 0.5, y_pred, 1.0)\n",
        "y_pred = jnp.where(y_pred >= 0.5, y_pred, 0.0)\n",
        "\n",
        "#Printing classification report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I61yhUSy5cOQ",
        "outputId": "afe75d0a-4703-45bb-a8c0-11324fd4eb6e"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00       834\n",
            "           1       1.00      1.00      1.00       833\n",
            "           2       1.00      1.00      1.00       833\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      2500\n",
            "   macro avg       1.00      1.00      1.00      2500\n",
            "weighted avg       1.00      1.00      1.00      2500\n",
            " samples avg       1.00      1.00      1.00      2500\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training with optax"
      ],
      "metadata": {
        "id": "r-7W0OeuPqDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using adam as an optimizer\n",
        "opt_adam = optax.adam(learning_rate=lr)\n",
        "opt_state = opt_adam.init(params)"
      ],
      "metadata": {
        "id": "2XVG5VxL5HfG"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = model.init(key, xs)  \n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss, grads = value_and_grad_fn(params)\n",
        "    updates, opt_state = opt_adam.update(grads, opt_state)  \n",
        "    params = optax.apply_updates(params, updates)\n",
        "    if epoch % log_period_epoch == 0:\n",
        "        print(f'epoch {epoch}, loss = {loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SMFviP83jyX",
        "outputId": "8f641795-ef5d-4acd-fddd-d5131088a414"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss = 2.889653205871582\n",
            "epoch 5, loss = 0.6834388375282288\n",
            "epoch 10, loss = 0.2309263050556183\n",
            "epoch 15, loss = 0.10921990871429443\n",
            "epoch 20, loss = 0.05437207594513893\n",
            "epoch 25, loss = 0.031396765261888504\n",
            "epoch 30, loss = 0.02037673443555832\n",
            "epoch 35, loss = 0.014530753716826439\n",
            "epoch 40, loss = 0.011463300324976444\n",
            "epoch 45, loss = 0.009741807356476784\n",
            "epoch 50, loss = 0.008639251813292503\n",
            "epoch 55, loss = 0.007859257981181145\n",
            "epoch 60, loss = 0.00728395814076066\n",
            "epoch 65, loss = 0.006848851218819618\n",
            "epoch 70, loss = 0.00650702603161335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting output of test dataset\n",
        "y_pred = model.apply(params, X_test)\n",
        "\n",
        "#Rounding off the probabilities\n",
        "y_pred = jnp.where(y_pred < 0.5, y_pred, 1.0)\n",
        "y_pred = jnp.where(y_pred >= 0.5, y_pred, 0.0)\n",
        "\n",
        "#Printing classification report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsRgygU_5LLY",
        "outputId": "2f45ecb1-882e-410b-e20b-184721bbd34f"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       834\n",
            "           1       1.00      1.00      1.00       833\n",
            "           2       1.00      1.00      1.00       833\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      2500\n",
            "   macro avg       1.00      1.00      1.00      2500\n",
            "weighted avg       1.00      1.00      1.00      2500\n",
            " samples avg       1.00      1.00      1.00      2500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Important References"
      ],
      "metadata": {
        "id": "99diNpx6btZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Youtube References\n",
        "1. [MC Dropout](https://youtu.be/eHT0raFtl1Q?t=181)\n",
        "2. [Machine Learning with FLAX - FROM ZERO TO HERO](https://youtu.be/5eUSmJvK8WA)\n",
        "3. [Future of ML research in JAX/FLAX](https://youtu.be/7Zau-5ozWfg)\n",
        "4. [MLP Mixer in Flax and Pytorch](https://youtu.be/HqytB2GUbHA)\n",
        "\n",
        "Python Notebooks\n",
        "5. [FLAX Linen tutorial](https://colab.research.google.com/github/google/flax/blob/main/docs/notebooks/linen_intro.ipynb)\n",
        "6. [FLAX basics](https://colab.research.google.com/github/BertrandRdp/flax/blob/master/docs/notebooks/flax_basics.ipynb)"
      ],
      "metadata": {
        "id": "qXNE32YNeoU_"
      }
    }
  ]
}